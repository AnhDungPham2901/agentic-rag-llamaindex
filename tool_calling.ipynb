{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba9fc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba551ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38e01f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two integers together.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "def mystery(x: int, y: int) -> int: \n",
    "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
    "    return (x + y) * (x + y)\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "116256cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
      "=== Function Output ===\n",
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystery_tool], \n",
    "    \"Tell me the output of the mystery function on 2 and 9\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb723ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"The Era of Experience Paper.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77a567cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1687cb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: The Era of Experience Paper.pdf\n",
      "file_path: The Era of Experience Paper.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 226196\n",
      "creation_date: 2025-04-21\n",
      "last_modified_date: 2025-04-21\n",
      "\n",
      "Welcome to the Era of Experience\n",
      "David Silver, Richard S. Sutton*\n",
      "Abstract\n",
      "We stand on the threshold of a new era in artificial intelligence that promises to achieve an unprece-\n",
      "dented level of ability. A new generation of agents will acquire superhuman capabilities by learning pre-\n",
      "dominantly from experience. This note explores the key characteristics that will define this upcoming era.\n",
      "The Era of Human Data\n",
      "Artificial intelligence (AI) has made remarkable strides over recent years by training on massive amounts of\n",
      "human-generated data and fine-tuning with expert human examples and preferences. This approach is exem-\n",
      "plified by large language models (LLMs) that have achieved a sweeping level of generality. A single LLM\n",
      "can now perform tasks spanning from writing poetry and solving physics problems to diagnosing medical\n",
      "issues and summarising legal documents.\n",
      "However, while imitating humans is enough to reproduce many human capabilities to a competent level,\n",
      "this approach in isolation has not and likely cannot achieve superhuman intelligence across many important\n",
      "topics and tasks. In key domains such as mathematics, coding, and science, the knowledge extracted from\n",
      "human data is rapidly approaching a limit. The majority of high-quality data sources - those that can actually\n",
      "improve a strong agent’s performance - have either already been, or soon will be consumed. The pace of\n",
      "progress driven solely by supervised learning from human data is demonstrably slowing, signalling the need\n",
      "for a new approach. Furthermore, valuable new insights, such as new theorems, technologies or scientific\n",
      "breakthroughs, lie beyond the current boundaries of human understanding and cannot be captured by existing\n",
      "human data.\n",
      "The Era of Experience\n",
      "To progress significantly further, a new source of data is required. This data must be generated in a way that\n",
      "continually improves as the agent becomes stronger; any static procedure for synthetically generating data\n",
      "will quickly become outstripped. This can be achieved by allowing agents to learn continually from their\n",
      "own experience, i.e., data that is generated by the agent interacting with its environment. AI is at the cusp of\n",
      "a new period in which experience will become the dominant medium of improvement and ultimately dwarf\n",
      "the scale of human data used in today’s systems.\n",
      "This transition may have already started, even for the large language models that epitomise human-centric\n",
      "AI. One example is in the capability of mathematics. AlphaProof [20] recently became the first program to\n",
      "achieve a medal in the International Mathematical Olympiad, eclipsing the performance of human-centric\n",
      "approaches [27, 19]. Initially exposed to around a hundred thousand formal proofs, created over many years\n",
      "*This is a preprint of a chapter that will appear in the book Designing an Intelligence, published by MIT Press.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b70d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1033fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"Who is the author of the paper?\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "273d4179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author of the paper is the AI community.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da1bf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'The Era of Experience Paper.pdf', 'file_path': 'The Era of Experience Paper.pdf', 'file_type': 'application/pdf', 'file_size': 226196, 'creation_date': '2025-04-21', 'last_modified_date': '2025-04-21'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d7d25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "\n",
    "\n",
    "def vector_query(\n",
    "    query: str, \n",
    "    page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Perform a vector search over an index.\n",
    "    \n",
    "    query (str): the string query to be embedded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
    "        over all pages. Otherwise, filter by the set of specified pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "    \n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_tool\",\n",
    "    fn=vector_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37a5f0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"main idea\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "The main idea is about the potential of experiential learning in AI systems, emphasizing the benefits of agents learning from continuous streams of experience rather than isolated interactions. It discusses how agents can adapt and improve over time by carrying information across their entire experience, enabling them to plan for future goals and achieve long-term outcomes. The focus is on harnessing the power of experiential data to break through the limitations of current human-centric AI systems and advance towards creating truly superhuman agents.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool], \n",
    "    \"What is the main idea described in paper 2?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6566a22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'The Era of Experience Paper.pdf', 'file_path': 'The Era of Experience Paper.pdf', 'file_type': 'application/pdf', 'file_size': 226196, 'creation_date': '2025-04-21', 'last_modified_date': '2025-04-21'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0d1ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful if you want to get a summary of a particular document. \"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a111f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"Actions vs Observations\", \"page_numbers\": [\"3\"]}\n",
      "=== Function Output ===\n",
      "Agents in the era of experience will act autonomously in the real world, interacting with their environment through various actions such as using user interfaces, executing code, calling APIs, monitoring sensors, operating equipment remotely, and conducting experiments. These actions enable the agents to explore, adapt to changing environments, and discover strategies independently. In contrast, observations in this era involve the agent analyzing real-world data, running simulations, suggesting experiments, and interventions to maximize long-term success towards a specified goal.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"Actions vs Observations described in page 3?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ed800bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '3', 'file_name': 'The Era of Experience Paper.pdf', 'file_path': 'The Era of Experience Paper.pdf', 'file_type': 'application/pdf', 'file_size': 226196, 'creation_date': '2025-04-21', 'last_modified_date': '2025-04-21'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74848752",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected at least one tool call, but got 0 tool calls.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_and_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mvector_query_tool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_tool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the summary of the document?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learning/agentic-rag-llamaindex/.venv/lib/python3.13/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    325\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learning/agentic-rag-llamaindex/.venv/lib/python3.13/site-packages/llama_index/core/llms/function_calling.py:190\u001b[39m, in \u001b[36mFunctionCallingLLM.predict_and_call\u001b[39m\u001b[34m(self, tools, user_msg, chat_history, verbose, allow_parallel_tool_calls, error_on_no_tool_call, error_on_tool_error, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().predict_and_call(\n\u001b[32m    175\u001b[39m         tools,\n\u001b[32m    176\u001b[39m         user_msg=user_msg,\n\u001b[32m   (...)\u001b[39m\u001b[32m    179\u001b[39m         **kwargs,\n\u001b[32m    180\u001b[39m     )\n\u001b[32m    182\u001b[39m response = \u001b[38;5;28mself\u001b[39m.chat_with_tools(\n\u001b[32m    183\u001b[39m     tools,\n\u001b[32m    184\u001b[39m     user_msg=user_msg,\n\u001b[32m   (...)\u001b[39m\u001b[32m    188\u001b[39m     **kwargs,\n\u001b[32m    189\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m tool_calls = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_tool_calls_from_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_no_tool_call\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_on_no_tool_call\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m tool_outputs = [\n\u001b[32m    194\u001b[39m     call_tool_with_selection(tool_call, tools, verbose=verbose)\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m tool_calls\n\u001b[32m    196\u001b[39m ]\n\u001b[32m    197\u001b[39m tool_outputs_with_error = [\n\u001b[32m    198\u001b[39m     tool_output \u001b[38;5;28;01mfor\u001b[39;00m tool_output \u001b[38;5;129;01min\u001b[39;00m tool_outputs \u001b[38;5;28;01mif\u001b[39;00m tool_output.is_error\n\u001b[32m    199\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learning/agentic-rag-llamaindex/.venv/lib/python3.13/site-packages/llama_index/llms/openai/base.py:964\u001b[39m, in \u001b[36mOpenAI.get_tool_calls_from_response\u001b[39m\u001b[34m(self, response, error_on_no_tool_call, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tool_calls) < \u001b[32m1\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m error_on_no_tool_call:\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    965\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected at least one tool call, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tool_calls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tool calls.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    966\u001b[39m         )\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    968\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[31mValueError\u001b[39m: Expected at least one tool call, but got 0 tool calls."
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"What is the summary of the document?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca5b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
